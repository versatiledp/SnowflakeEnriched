
<!DOCTYPE html>

<html>
  <head><link rel="shortcut icon" type="image/x-icon" href="SnowflakeEnriched.ico" />
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Introducing data platform &#8212; Contents v1.0 documentation</title>
    <link rel="stylesheet" href="static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="static/my.css" type="text/css" />
    <link rel="stylesheet" href="static/copybutton.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="static/documentation_options.js"></script>
    <script src="static/jquery.js"></script>
    <script src="static/underscore.js"></script>
    <script src="static/doctools.js"></script>
    <script src="static/language_data.js"></script>
    <script src="static/clipboard.min.js"></script>
    <script src="static/copybutton.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="CSV ingestion" href="CSVIngestion.html" />
    <link rel="prev" title="Introducing Snowflake" href="IntroducingSnowflake.html" />
   
  <link rel="stylesheet" href="static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="introducing-data-platform">
<h1>Introducing data platform<a class="headerlink" href="#introducing-data-platform" title="Permalink to this headline">¶</a></h1>
<p>The modern data architecture allows enterprises to ingest data coming from
multiple systems, in a variety of data formats, at different speeds, and at unknown
intervals. The layered data platform design makes it easy to process big
data efficiently. It enables organizations to quickly deploy new analytical
business-driven solutions to drive revenue and
profitability.</p>
<div class="section" id="pillars-of-data-platform">
<h2>Pillars of data platform<a class="headerlink" href="#pillars-of-data-platform" title="Permalink to this headline">¶</a></h2>
<p>There are four main pillars of the modern data platform:</p>
<ul class="simple">
<li><p>Data source layer</p></li>
<li><p>Data processing and storage layer</p></li>
<li><p>Analytics layer</p></li>
<li><p>Consumption layer</p></li>
</ul>
<div class="figure align-center">
<img alt="images/DataPlatform.jpg" src="images/DataPlatform.jpg" />
</div>
<div class="section" id="data-source-layer">
<h3>Data source layer<a class="headerlink" href="#data-source-layer" title="Permalink to this headline">¶</a></h3>
<p>Data source layer receives data from various sources.
Each source can be either internal or external to an organization
and generates data in real-time or in batch mode.
The <strong>variety</strong> of data formats can be structured, semi-structured, or unstructured.
The <strong>velocity</strong> (speed of arrival) and <strong>volume</strong> (delivery amount) will differ by source.</p>
</div>
<div class="section" id="data-processing-and-storage-layer">
<h3>Data processing and storage layer<a class="headerlink" href="#data-processing-and-storage-layer" title="Permalink to this headline">¶</a></h3>
<p>The data processing layer receives data from the data sources, converts the data into a comprehensible
format for the data servicing and analytics tool, and stores the
data. For example, large amounts of data are
stored in the Hadoop distributed file system store (HDFS). Large data
processing is performed through Hadoop/Spark systems. Data may
undergo format changes as it is processed through these systems. Cloud
service providers like Amazon, Google, and Microsoft allows to build and
operate data-centric applications on an infinite scale. Robust and
inexpensive storage is fundamental to the operation and scalability of the
big data architecture.</p>
<p>BigQuery, Azure Synapse, Amazon Redshift, and Snowflake
are used as standalone solutions for big data processing, or in
combination with the Hadoop/Spark ecosystems.</p>
</div>
<div class="section" id="analytical-layer">
<h3>Analytical layer<a class="headerlink" href="#analytical-layer" title="Permalink to this headline">¶</a></h3>
<p>The Analytical layer reads the data ingested and transformed by the data
processing and storage layer of the big data ecosystem. This layer consists of a variety of data
for different user requirements and
provides data discovery mechanisms on large volumes of
data. Apache Spark SQL, Hive, Apache Spark Streaming, Apache Spark GraphX, machine learning
libraries, SQL libraries, and a number of other toolsets
are utilized in this layer to understand the underlying data landscape.</p>
</div>
<div class="section" id="consumption-layer">
<h3>Consumption layer<a class="headerlink" href="#consumption-layer" title="Permalink to this headline">¶</a></h3>
<p>The consumption layer is also called the business intelligence layer. This layer
receives results from the analytical layer and presents the results
using visualization tools and
business processes.</p>
</div>
</div>
<div class="section" id="project-data-source">
<h2>Project data source<a class="headerlink" href="#project-data-source" title="Permalink to this headline">¶</a></h2>
<p>This book is designed to use data from the Integrated Postsecondary Education Data System (IPEDS)
and build analytics echo system using this data.</p>
<p>The National Center for Education Statistics (NCES) is the primary federal entity for collecting
and analyzing data related to education in the U.S. and other nations.
NCES is located within the U.S. Department of Education and the Institute of Education Sciences.
NCES fulfills a Congressional mandate to collect, collate, analyze, and report complete statistics
on the condition of American education; conduct and publish reports; and review and report on
education activities internationally. The Integrated Postsecondary Education Data System (IPEDS),
established as the core postsecondary education data collection program for NCES.
IPEDS  <a class="reference external" href="https://nces.ed.gov/ipeds"  target="_blank">https://nces.ed.gov/ipeds</a>
is the primary source for information on U.S. Colleges,
universities, technical institutions, and vocational institutions.</p>
<div class="figure align-center">

</div>
<p>All of the data used in this book is downloaded from <a class="reference external" href="https://nces.ed.gov/ipeds/use-the-data"   target="_blank">https://nces.ed.gov/ipeds/use-the-data</a>.</p>
<p>From IPEDS, we are downloading the files listed below. Data is downloaded for the three academic years (2017, 2018, and 2019).</p>
<p><strong>Academic institution [HDR]</strong></p>
<p>These files contain directory information for every institution in the IPEDS universe.
The files include the name, address, city, state, zip code, and various URL links to the institution's home page,
admissions, and financial aid offices, and net price calculator.</p>
<p><strong>Enrollment 12-Month [EFFY]</strong></p>
<p>These files contain the unduplicated headcount of students enrolled over a
12-month period for both undergraduate and graduate levels. Each record is
uniquely defined by the variables IPEDS, ID, and the level of enrollment.</p>
<p><strong>Institutional charges [IC_AY]</strong></p>
<p>These files contain data on student charges for a full academic year. The price of attendance includes the
published tuition and required fees, books and supplies, room and board, and other expenses.</p>
<p><strong>Admissions and test scores [ADM]</strong></p>
<p>These files contain information about the undergraduate selection process
for first-time, degree/certificate-seeking prospective students.</p>
<p><strong>Code mapping data</strong></p>
<p>These files contain data for lookup tables.
A lookup table acts as a dictionary. It provides descriptions for a business codes.
Few examples are institute type description for given institute type code,
state name given state code, etc..</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><em>Disclaimer: We have modified this data to explore Snowflake functionality. Therefore, analytics results from this book
should not be used as a replacement for the original data and its results.
Please refer to the NCES website for actual statistics and research on academic institutions.</em></p>
<p><em>Utilities to convert the CSV files into different data formats are available at:</em>
<a class="reference external" href="https://github.com/versatiledp/SnowflakeEnriched/tree/master/utilities"   target="_blank">https://github.com/versatiledp/SnowflakeEnriched/tree/master/utilities</a></p>
<p><em>All the data files used throughout this book are available to download at:</em>
<a class="reference external" href="https://github.com/versatiledp/SnowflakeEnriched/tree/master/data/input"  target="_blank">https://github.com/versatiledp/SnowflakeEnriched/tree/master/data/input</a></p>
</div>
<p>These files are coming to the data platform in the format like CSV, JSON, ORC, Parquet, and Multi-format.</p>
<div class="section" id="academic-institution-hdr">
<h3>Academic institution [HDR]<a class="headerlink" href="#academic-institution-hdr" title="Permalink to this headline">¶</a></h3>
<p>These files are available in CSV format at the source system.
The institutional characteristics header files are directly ingested into
Snowflake staging from the source system. The following diagram displays
data flow between the source system and the Snowflake destination.</p>
<div class="figure align-center">
<img alt="images/csvIngestion.jpg" src="images/csvIngestion.jpg" />
</div>
<p>ELT layer defines the schema for the CSV file with data types.
The OD layer is used to process data for the destination layer with additional transformations.</p>
</div>
<div class="section" id="enrollment-12-month-effy">
<h3>Enrollment 12-Month [EFFY]<a class="headerlink" href="#enrollment-12-month-effy" title="Permalink to this headline">¶</a></h3>
<p>EFFY data files are in JSON format. JSON is a common open standard file and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute-value pairs and array data types. This data format is used in a diverse range of applications and serves as a replacement for XML in AJAX systems.
In our use-case, we are ingesting enrollment (EFFY) files in the JSON format directly to Snowflake.
The following diagram displays the data ingestion architecture for JSON files.</p>
<div class="figure align-center">
<img alt="images/jsonIngestion.jpg" src="images/jsonIngestion.jpg" />
</div>
<p>In this book, we utilised Python programming to create JSON file used for simulation. Python code is used to convert CSV files from IPEDS universe to JSON format.</p>
<p>We convert JSON data into relational table structure in OD tables of Snowflake transformation.
OD layer is used to process data for the destination layer with additional transformation.</p>
</div>
<div class="section" id="institutional-charges-ic">
<h3>Institutional charges [IC]<a class="headerlink" href="#institutional-charges-ic" title="Permalink to this headline">¶</a></h3>
<p>IC_AY files are available in ORC format. The Optimized Row Columnar (ORC) file format provides a highly efficient way to store Hive data. ORC files improve performance when Hive is reading, writing, and processing data.
The compact nature of this format is ideal, and enables skipping over irrelevant parts without needing large, complex, or manually maintained indices.</p>
<p>The following diagram displays the data ingestion architecture for ORC format files. It provides view around
major Hadoop implementations.</p>
<div class="figure align-center">
<img alt="images/orcIngestion.jpg" src="images/orcIngestion.jpg" />
</div>
<p>Data from different source systems are brought to Hadoop enviornment for big data processing. We would use ORC files
coming out of Hadoop and ingest into Snowflake.</p>
<p>To create ORC file for simulation in this book, we have used second python convertor.
This python code migrated CSV files from IPEDS universe to
ORC format.</p>
<p>ORC data is converted into relational table structure in OD tables of Snowflake transformation.
OD layer is used to process data for the destination layer with additional transformation.</p>
</div>
<div class="section" id="admissions-and-test-scores-adm">
<h3>Admissions and test scores [ADM]<a class="headerlink" href="#admissions-and-test-scores-adm" title="Permalink to this headline">¶</a></h3>
<p>Admission data (ADM) files are available in the Parquet data format.
Parquet, an open-source file format for Hadoop stores nested data structures in a flat columnar format.
Compared to a traditional approach where data is stored in a row-oriented approach, Parquet is more efficient
in terms of storage and performance. It is especially useful for queries that read specific columns
from a wide table. Parquet provides optimizations to speed up queries.
The following diagram lays out the architecture from data source to Parquet ingestion.</p>
<div class="figure align-center">
<img alt="images/parquetIngestion.jpg" src="images/parquetIngestion.jpg" />
</div>
<p>Apache Spark has become the de-facto standard for big data processing recently. Spark output is written in
Parquet format.</p>
<p>For simulating Parquet data, we have written yet another utility to convert CSV files from IPEDS universe
to Parquet files.</p>
<p>We convert Parquet data into relational table structure in OD tables of Snowflake transformation.
OD layer is used to process data for the destination layer with additional transformation.</p>
</div>
<div class="section" id="code-mapping-data">
<h3>Code mapping data<a class="headerlink" href="#code-mapping-data" title="Permalink to this headline">¶</a></h3>
<p>This Multi-format dataset has a special character delimiter. One file holds data for multiple destination tables, which are
identified by the value of the first column in the file.</p>
<p>These files are directly ingested into Snowflake staging from the source system.
Multiple OD tables will be populated from one lookup file.</p>
<p>Download the files from GitRepo <a class="reference external" href="https://github.com/versatiledp/SnowflakeEnriched/tree/master/data/input"  target="_blank">https://github.com/versatiledp/SnowflakeEnriched/tree/master/data/input</a> to local machine.
Create folder <strong>C:\Snowflake\data\input\</strong> and copy downloaded files to this folder.
The files in this book are loaded from the above folder to the Snowflake stage area using SnowSQL CLI.</p>
</div>
</div>
<div class="section" id="snowflake-data-pillers">
<h2>Snowflake data pillers<a class="headerlink" href="#snowflake-data-pillers" title="Permalink to this headline">¶</a></h2>
<p>For the project in this book, we will be using Snowflake staged files, transient tables, and master tables.
Transient tables are referenced as operational data tables through out the book.
Master tables are also called as the servicing layer entities.
Snowflake staged files, transient tables along with the source system
represent data source layer.
Stored procedures are used for data processing.
Storage and analytical layer is implemented in the transformed servicing entities.
Google Colab is used for the consumption layer.</p>
<p>The project work in this book, we seek to:</p>
<ul class="simple">
<li><p>Ingest variety of data formats.</p></li>
<li><p>Transform data for business users.</p></li>
<li><p>Build a data pipeline to process data on a periodic basis.</p></li>
<li><p>Create descriptive analytics for use-cases laid out by businesses.</p></li>
</ul>
<div class="section" id="staged-file">
<h3>Staged file<a class="headerlink" href="#staged-file" title="Permalink to this headline">¶</a></h3>
<p>All the data from the source system will be brought into the internal staged file location of Snowflake.
This data is directly from transactional systems or existing big data platforms,
with no transformations applied.</p>
</div>
<div class="section" id="operational-datastore-od">
<h3>Operational datastore (OD)<a class="headerlink" href="#operational-datastore-od" title="Permalink to this headline">¶</a></h3>
<p>An operational datastore (OD) is a central store that provides a snapshot of
the latest data from multiple transactional systems.
It enables organizations to combine data in its original format
from various sources into a single destination to make it available for further processing.
These are initial physical tables created in Snowflake and consists of data
that has undergone little to no transformation.
The OD table structure very closely resembles that of the staged file with proper data types and column names, alongside
additional columns to help track data lineage. The OD also serves as transient place to store data coming from
the source systems. It allows us have all the data within the bounds ready for processing and modeling</p>
</div>
<div class="section" id="servicing-layer">
<h3>Servicing layer<a class="headerlink" href="#servicing-layer" title="Permalink to this headline">¶</a></h3>
<p>This layer consists of the data that has undergone all sorts of transformations and it is
ready for consumption to the business intelligence layer.
OD tables serve as input to the transformation process to populate servicing layer tables.
This is our master schema with transformed data. It contains correctly modeled tables,
that are named properly with business centeric column names with appropriate data types.</p>
</div>
<div class="section" id="dataflow-summary">
<h3>Dataflow summary<a class="headerlink" href="#dataflow-summary" title="Permalink to this headline">¶</a></h3>
<p>The data cleaning and transformation takes place while moving data from OD into service layer.</p>
<ul class="simple">
<li><p>Standardising all date formats and time zones.</p></li>
<li><p>Rounding numbers.</p></li>
<li><p>Cleaning strings and character data.</p></li>
<li><p>Standardising data to be of the same format.</p></li>
<li><p>Splitting out data into multiple columns.</p></li>
<li><p>Extracting data from CSV, JSON, ORC, and Parquet.</p></li>
<li><p>Combining data from multiple columns to Single JSON objects.</p></li>
</ul>
<div class="figure align-center">

</div>
<p>The diagram below shows the data flow process between files and the final destination in Snowflake.</p>
<div class="figure align-center">
<img alt="images/ingestionlayout.jpg" src="images/ingestionlayout.jpg" />
</div>
<p>Not all data from OD tables may land up in the servicing layer.
Normally, business users like to use tools such as PowerBI and Tableau to
perform raw queries on the datasets. The best place for business users
to perform these analytics is the service layer.</p>
<p>In data science, data attributes from the staged and OD layer may be needed to build predictive analytics.
Thus, access can be granted to OD and service layer entities.</p>
<p>Data engineering consists of creating transformations, performing metadata management, and
keeping track of data pipelines. So, access should be
granted to all layers of the data platform.</p>
</div>
</div>
<div class="section" id="ingesting-snowflake-stage">
<h2>Ingesting Snowflake stage<a class="headerlink" href="#ingesting-snowflake-stage" title="Permalink to this headline">¶</a></h2>
<p>Data files are received in various formats like CSV, ORC, Parquet, and JSON. These files are available in the folders by year. They are brought into Snowflake staged files using the PUT command.
Each staged file contains the entire data for that specific year. From these staged files, OD tables are populated.
Using these OD tables, servicing layer tables of the data-warehouse are populated. Snowflake's stored procedures, views, and functions are used for ETL/ELT processes.</p>
<p>Internal named stage needs to be created for each of the file formats before loading data.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<dl>
<dt><em>All the Snowflake SQL commands in this book are to be executed from Snowflake worksheet unless specified differently</em>.</dt><dd><div class="line-block">
<div class="line"><em>Login to the Snowflake account</em>.</div>
<div class="line"><em>Use IPEDS database</em>.</div>
<div class="line"><em>Copy the code from book and execute</em>.</div>
</div>
</dd>
</dl>
</div>
<div class="section" id="create-internal-stage-area-ipeds">
<h3>Create internal stage area IPEDS<a class="headerlink" href="#create-internal-stage-area-ipeds" title="Permalink to this headline">¶</a></h3>
<p>First, create the file formats and stage area for the following data formats:</p>
<blockquote>
<div><ul class="simple">
<li><p>Tab delimited</p></li>
<li><p>Comma separated with a header record</p></li>
<li><p>Comma separated with no header</p></li>
</ul>
</div></blockquote>
<div class="figure align-center">

</div>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">--</span> <span class="n">Number</span> <span class="n">of</span> <span class="n">header</span> <span class="n">lines</span> <span class="n">at</span> <span class="n">the</span> <span class="n">start</span> <span class="n">of</span> <span class="n">the</span> <span class="n">file</span><span class="o">.</span>
<span class="o">--</span> <span class="n">The</span> <span class="n">COPY</span> <span class="n">command</span> <span class="n">skips</span> <span class="n">header</span> <span class="n">lines</span> <span class="n">when</span> <span class="n">loading</span> <span class="n">data</span>
<span class="o">--</span> <span class="n">All</span> <span class="n">these</span> <span class="n">file</span> <span class="n">formats</span> <span class="n">can</span> <span class="n">be</span> <span class="n">used</span> <span class="n">to</span> <span class="n">create</span> <span class="n">different</span> <span class="n">stage</span> <span class="n">areas</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span>  <span class="n">FILE</span> <span class="n">FORMAT</span>
                <span class="n">ff_IPEDS_CSVSkipHeaderTabDelimited</span>
                        <span class="n">TYPE</span> <span class="o">=</span>   <span class="n">CSV</span>
                        <span class="n">FIELD_DELIMITER</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span>  <span class="n">COMPRESSION</span> <span class="o">=</span> <span class="n">AUTO</span>
                        <span class="n">SKIP_HEADER</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span>  <span class="n">FILE</span> <span class="n">FORMAT</span>
                <span class="n">ff_IPEDS_CSVSkipHeaderCommaDelimited</span>
                        <span class="n">TYPE</span> <span class="o">=</span>   <span class="n">CSV</span>
                        <span class="n">FIELD_DELIMITER</span> <span class="o">=</span> <span class="s1">&#39;,&#39;</span>  <span class="n">COMPRESSION</span> <span class="o">=</span> <span class="n">AUTO</span>
                        <span class="n">SKIP_HEADER</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="o">--</span> <span class="n">The</span> <span class="n">COPY</span> <span class="n">command</span> <span class="n">does</span> <span class="ow">not</span> <span class="n">skip</span> <span class="nb">any</span> <span class="n">lines</span><span class="o">.</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span>  <span class="n">FILE</span> <span class="n">FORMAT</span>
                <span class="n">ff_IPEDS_CSVHeaderTabDelimited</span>
                        <span class="n">TYPE</span> <span class="o">=</span>   <span class="n">CSV</span>
                        <span class="n">FIELD_DELIMITER</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span>  <span class="n">COMPRESSION</span> <span class="o">=</span> <span class="n">AUTO</span>
                        <span class="n">SKIP_HEADER</span> <span class="o">=</span> <span class="mi">0</span> <span class="p">;</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span>  <span class="n">FILE</span> <span class="n">FORMAT</span>
                <span class="n">ff_IPEDS_CSVHeaderCommaDelimited</span>
                        <span class="n">TYPE</span> <span class="o">=</span>   <span class="n">CSV</span>
                        <span class="n">FIELD_DELIMITER</span> <span class="o">=</span> <span class="s1">&#39;,&#39;</span>  <span class="n">COMPRESSION</span> <span class="o">=</span> <span class="n">AUTO</span>
                        <span class="n">SKIP_HEADER</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">encoding</span> <span class="o">=</span> <span class="s1">&#39;iso-8859-1&#39;</span>
                        <span class="n">FIELD_OPTIONALLY_ENCLOSED_BY</span><span class="o">=</span><span class="s1">&#39;&quot;&#39;</span><span class="p">;</span>
<span class="o">--</span> <span class="n">create</span> <span class="n">satge</span> <span class="n">area</span> <span class="n">to</span> <span class="n">load</span> <span class="n">csv</span> <span class="n">files</span> <span class="k">for</span> <span class="n">the</span> <span class="n">project</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span> <span class="n">STAGE</span> <span class="n">IPEDS_HD</span> <span class="n">FILE_FORMAT</span>
                        <span class="o">=</span> <span class="n">ff_IPEDS_CSVHeaderCommaDelimited</span><span class="p">;</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span> <span class="n">STAGE</span> <span class="n">IPEDS_CM</span> <span class="n">FILE_FORMAT</span>
                        <span class="o">=</span> <span class="n">ff_IPEDS_CSVSkipHeaderTabDelimited</span><span class="p">;</span>
</pre></div>
</div>
<p>— Next, create the JSON file format and stage area.</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">--</span> <span class="n">Create</span> <span class="n">file</span> <span class="nb">format</span> <span class="n">to</span> <span class="n">load</span> <span class="n">JSON</span> <span class="n">file</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span> <span class="n">FILE</span> <span class="n">FORMAT</span> <span class="n">ff_IPEDS_Json</span>
                <span class="n">TYPE</span> <span class="o">=</span><span class="n">JSON</span> <span class="n">TRIM_SPACE</span> <span class="o">=</span> <span class="n">TRUE</span><span class="p">;</span>
<span class="o">--</span> <span class="n">Create</span> <span class="n">stage</span> <span class="n">area</span> <span class="n">to</span> <span class="n">load</span> <span class="n">JSON</span> <span class="n">file</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span> <span class="n">stage</span> <span class="n">IPEDS_EFFY</span> <span class="n">FILE_FORMAT</span> <span class="o">=</span> <span class="n">ff_IPEDS_Json</span><span class="p">;</span>
</pre></div>
</div>
<p>— Then, create the ORC file format and stage area.</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">--</span> <span class="n">Create</span> <span class="n">file</span> <span class="nb">format</span> <span class="n">to</span> <span class="n">load</span> <span class="n">ORC</span> <span class="n">file</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span> <span class="n">FILE</span> <span class="n">FORMAT</span> <span class="n">ff_IPEDS_ORC</span>
                        <span class="n">TYPE</span> <span class="o">=</span><span class="n">ORC</span> <span class="n">TRIM_SPACE</span> <span class="o">=</span> <span class="n">TRUE</span><span class="p">;</span>
<span class="o">--</span> <span class="n">Create</span> <span class="n">stage</span> <span class="n">area</span> <span class="n">to</span> <span class="n">load</span> <span class="n">ORC</span> <span class="n">file</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span> <span class="n">stage</span> <span class="n">IPEDS_IC</span> <span class="n">FILE_FORMAT</span> <span class="o">=</span> <span class="n">ff_IPEDS_ORC</span><span class="p">;</span>
</pre></div>
</div>
<p>— Finally, create the Parquet file format and stage area.</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">--</span> <span class="n">Create</span> <span class="n">Parquet</span> <span class="n">file</span> <span class="nb">format</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span>  <span class="n">FILE</span> <span class="n">FORMAT</span> <span class="n">ff_IPEDS_Parquet</span>
                        <span class="n">TYPE</span> <span class="o">=</span><span class="n">PARQUET</span> <span class="n">TRIM_SPACE</span> <span class="o">=</span> <span class="n">TRUE</span><span class="p">;</span>
<span class="o">--</span> <span class="n">Create</span> <span class="n">stage</span> <span class="n">area</span> <span class="n">to</span> <span class="n">load</span> <span class="n">Parquet</span> <span class="n">file</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span> <span class="n">stage</span> <span class="n">IPEDS_ADM</span> <span class="n">FILE_FORMAT</span> <span class="o">=</span> <span class="n">ff_IPEDS_Parquet</span><span class="p">;</span>
</pre></div>
</div>
<p>Once these formats and stage areas are created,
we can validate their availability.</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">SHOW</span> <span class="n">STAGES</span> <span class="p">;</span>
<span class="n">SHOW</span> <span class="n">STAGES</span> <span class="n">LIKE</span> <span class="s1">&#39;%IPED%&#39;</span><span class="p">;</span>
</pre></div>
</div>
<div class="figure align-center">
<img alt="images/stagefilelist.JPG" src="images/stagefilelist.JPG" />
</div>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">SHOW</span> <span class="n">FILE</span> <span class="n">FORMATS</span><span class="p">;</span>
<span class="n">SHOW</span> <span class="n">FILE</span> <span class="n">FORMATS</span> <span class="n">LIKE</span> <span class="s1">&#39;%IPED%&#39;</span><span class="p">;</span>
</pre></div>
</div>
<div class="figure align-center">
<img alt="images/formats.JPG" src="images/formats.JPG" />
</div>
<p>At this point, we have created an internal named stage in the Snowflake to ingest CSV, Parquet, ORC, and JSON files.</p>
<p>We will use the following stage area to load the raw files.</p>
<blockquote>
<div><ul class="simple">
<li><p>IPEDS_ADM</p></li>
<li><p>IPEDS_EFFY</p></li>
<li><p>IPEDS_CM</p></li>
<li><p>IPEDS_HD</p></li>
<li><p>IPEDS_IC</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="load-the-files-to-internal-stage-for-ipeds">
<h3>Load the files to internal stage for IPEDS<a class="headerlink" href="#load-the-files-to-internal-stage-for-ipeds" title="Permalink to this headline">¶</a></h3>
<p>In this section, we are ingesting all the files to the Snowflake staging area. We assume you have
downloaded the files from GitRepo and copied to the folder C:\Snowflake\data\input\.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<blockquote>
<div><div class="line-block">
<div class="line"><em>Right click on command promt application in windows machine and run as administrator</em></div>
<div class="line"><em>Change directory by executing CD C:\Snowflake\data\input from the command prompt</em></div>
<div class="line"><em>Check content of the folder with DIR command</em></div>
</div>
</div></blockquote>
<div class="figure align-center">
<img alt="images/inputfileList.JPG" src="images/inputfileList.JPG" />
</div>
<dl>
<dt><em>Connect to the Snowflake database using SnowSQL CLI</em>.</dt><dd><div class="line-block">
<div class="line"><em>Execute all the commands listed below in this section from SnowSQL CLI</em>.</div>
</div>
</dd>
</dl>
</div>
<p>Load the CSV academic institution (HD) files.</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">HD2017</span><span class="o">.</span><span class="n">csv</span> <span class="nd">@IPEDS_HD</span><span class="p">;</span>
<span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">HD2018</span><span class="o">.</span><span class="n">csv</span> <span class="nd">@IPEDS_HD</span><span class="p">;</span>
<span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">HD2019</span><span class="o">.</span><span class="n">csv</span> <span class="nd">@IPEDS_HD</span><span class="p">;</span>
</pre></div>
</div>
<p>Load the JSON enrollment (EFFY) files.</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">effy2017_rv</span><span class="o">.</span><span class="n">json</span> <span class="nd">@IPEDS_EFFY</span><span class="p">;</span>
<span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">effy2018_rv</span><span class="o">.</span><span class="n">json</span> <span class="nd">@IPEDS_EFFY</span><span class="p">;</span>
<span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">effy2019_rv</span><span class="o">.</span><span class="n">json</span> <span class="nd">@IPEDS_EFFY</span><span class="p">;</span>
</pre></div>
</div>
<p>Load the ORC institutional charges (IC)  files.
All the ORC partitions are loaded into a specific folder of ORC staging area.</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">ic2017_ay_orc</span>\<span class="o">*.*</span> <span class="nd">@IPEDS_IC</span><span class="o">/</span><span class="mi">2017</span><span class="o">/</span><span class="p">;</span>
<span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">ic2018_ay_orc</span>\<span class="o">*.*</span> <span class="nd">@IPEDS_IC</span><span class="o">/</span><span class="mi">2018</span><span class="o">/</span><span class="p">;</span>
<span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">ic2019_ay_orc</span>\<span class="o">*.*</span> <span class="nd">@IPEDS_IC</span><span class="o">/</span><span class="mi">2019</span><span class="o">/</span><span class="p">;</span>
</pre></div>
</div>
<p>Load the Parquet admission (ADM) files.
All the Parquet partitions are loaded into a specific folder of the Parquet staging area.</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">adm2017</span><span class="o">.</span><span class="n">parquet</span>\<span class="o">*.*</span> <span class="nd">@IPEDS_ADM</span><span class="o">/</span><span class="mi">2017</span><span class="o">/</span><span class="p">;</span>
<span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">adm2018</span><span class="o">.</span><span class="n">parquet</span>\<span class="o">*.*</span> <span class="nd">@IPEDS_ADM</span><span class="o">/</span><span class="mi">2018</span><span class="o">/</span><span class="p">;</span>
<span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">adm2019</span><span class="o">.</span><span class="n">parquet</span>\<span class="o">*.*</span> <span class="nd">@IPEDS_ADM</span><span class="o">/</span><span class="mi">2019</span><span class="o">/</span><span class="p">;</span>
</pre></div>
</div>
<p>Load the CSV code mapping (CM) files.</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">CodeMappingData</span><span class="o">.</span><span class="n">txt</span> <span class="nd">@IPEDS_CM</span><span class="p">;</span>
</pre></div>
</div>
<p>Once these files are loaded to the stage area,
we can check their availability in the internal stage area by executing the code from Snowflake worksheet area as follows:</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span> <span class="nd">@IPEDS_HD</span><span class="p">;</span>
</pre></div>
</div>
<div class="figure align-center">
<img alt="images/csvlist.JPG" src="images/csvlist.JPG" />
</div>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span> <span class="nd">@IPEDS_EFFY</span><span class="p">;</span>
</pre></div>
</div>
<div class="figure align-center">
<img alt="images/jsonlist.JPG" src="images/jsonlist.JPG" />
</div>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span> <span class="nd">@IPEDS_IC</span><span class="p">;</span>
</pre></div>
</div>
<div class="figure align-center">
<img alt="images/stagedORC.JPG" src="images/stagedORC.JPG" />
</div>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span> <span class="nd">@IPEDS_ADM</span><span class="p">;</span>
</pre></div>
</div>
<div class="figure align-center">
<img alt="images/Parquetlist.JPG" src="images/Parquetlist.JPG" />
</div>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span> <span class="nd">@IPEDS_CM</span><span class="p">;</span>
</pre></div>
</div>
<div class="figure align-center">
<img alt="images/csvOther.JPG" src="images/csvOther.JPG" />
</div>
<p>Terminate the current session as shown below.</p>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>!exit
</pre></div>
</div>
<p>All the files from the source system are ingested into the Snowflake staged area.
If there are any issues in the staged area, drop staged area using <strong>remove &#64;StageAreaName</strong>.
Example to drop ADM file stage area, execute the command remove &#64;IPEDS_ADM.
To recreate dropped staged area, we will need to re-run the
create staged area and load data file code again .</p>
</div>
</div>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>This chapter summarizes, data files, file formats, and processes used to simplify
and streamline bulk loading data into various
internal staging areas.
The data from the transactional source systems go through multiple data processing layers.
This results in a variety of data formats being created.
We map these data files into the analytical metadata layer. This chapter
summarized all the data formats we will be dealing with in the processing layer of the analytical data platform.</p>
<p>With this understanding of Snowflake and data platform overview, let us start ingesting
our first set of CSV files from the source system.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><em>SourceCode:</em>
<a class="reference external" href="https://github.com/versatiledp/SnowflakeEnriched/tree/master/source/code/SQL/StagedFile">https://versatiledp.github.io/SnowflakeEnriched/tree/master/source/code/SQL/StagedFile</a></p>
<div class="line-block">
<div class="line"><em>The Mapping between the source system and Snowflake metadata is at:</em></div>
<div class="line"><a class="reference external" href="https://versatiledp.github.io/SnowflakeEnriched/Mapping/AcademicInstitute.htm">https://versatiledp.github.io/SnowflakeEnriched/Mapping/AcademicInstitute</a></div>
<div class="line"><a class="reference external" href="https://versatiledp.github.io/SnowflakeEnriched/Mapping/Enrollment.htm">https://versatiledp.github.io/SnowflakeEnriched/Mapping/Enrollment</a></div>
<div class="line"><a class="reference external" href="https://versatiledp.github.io/SnowflakeEnriched/Mapping/InstitutionCharges.htm">https://versatiledp.github.io/SnowflakeEnriched/Mapping/InstitutionCharges</a></div>
<div class="line"><a class="reference external" href="https://versatiledp.github.io/SnowflakeEnriched/Mapping/AdmissionStat.htm">https://versatiledp.github.io/SnowflakeEnriched/Mapping/AdmissionStat</a></div>
<div class="line"><a class="reference external" href="https://versatiledp.github.io/SnowflakeEnriched/Mapping/CodeValue.htm">https://versatiledp.github.io/SnowflakeEnriched/Mapping/CodeValue</a></div>
</div>
</div>
<table class="docutils align-default">
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="./index.html">&nbsp;&nbsp;<img src= "images/tableofcontents.png" width="25px" height="20px"><font color="blue"><b>Table of contents</b> </font></a></p></td>
<td><p><a class="reference external" href="./IntroducingSnowflake.html">&nbsp;&nbsp;<img src= "images/previouschapter.png" width="30px" height="20px"><font color="blue"><b>Previous chapter</b> </font></a></p></td>
<td><p><a class="reference external" href="./CSVIngestion.html">&nbsp;&nbsp;<img src= "images/nextchapter.png" width="30px" height="20px"><font color="blue"><b>Next chapter</b> </font></a></p></td>
</tr>
</tbody>
</table>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Contents</a></h1>









<p class="caption"></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Preface.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="IntroducingSnowflake.html">Introducing Snowflake</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Introducing data platform</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#pillars-of-data-platform">Pillars of data platform</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#data-source-layer">Data source layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-processing-and-storage-layer">Data processing and storage layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#analytical-layer">Analytical layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#consumption-layer">Consumption layer</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#project-data-source">Project data source</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#academic-institution-hdr">Academic institution [HDR]</a></li>
<li class="toctree-l3"><a class="reference internal" href="#enrollment-12-month-effy">Enrollment 12-Month [EFFY]</a></li>
<li class="toctree-l3"><a class="reference internal" href="#institutional-charges-ic">Institutional charges [IC]</a></li>
<li class="toctree-l3"><a class="reference internal" href="#admissions-and-test-scores-adm">Admissions and test scores [ADM]</a></li>
<li class="toctree-l3"><a class="reference internal" href="#code-mapping-data">Code mapping data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#snowflake-data-pillers">Snowflake data pillers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#staged-file">Staged file</a></li>
<li class="toctree-l3"><a class="reference internal" href="#operational-datastore-od">Operational datastore (OD)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#servicing-layer">Servicing layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dataflow-summary">Dataflow summary</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#ingesting-snowflake-stage">Ingesting Snowflake stage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#create-internal-stage-area-ipeds">Create internal stage area IPEDS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#load-the-files-to-internal-stage-for-ipeds">Load the files to internal stage for IPEDS</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="CSVIngestion.html">CSV ingestion</a></li>
<li class="toctree-l1"><a class="reference internal" href="JSONIngestion.html">JSON ingestion</a></li>
<li class="toctree-l1"><a class="reference internal" href="ORCIngestion.html">ORC ingestion</a></li>
<li class="toctree-l1"><a class="reference internal" href="ParquetIngestion.html">Parquet ingestion</a></li>
<li class="toctree-l1"><a class="reference internal" href="SemiStructuredDataIngestion.html">Multi-format ingestion</a></li>
<li class="toctree-l1"><a class="reference internal" href="PipelineOrchestration.html">Workflow orchestration</a></li>
<li class="toctree-l1"><a class="reference internal" href="BusinessInsights.html">Business insights</a></li>
<li class="toctree-l1"><a class="reference internal" href="Summary.html">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="Abbreviations.html">Abbreviations</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="IntroducingSnowflake.html" title="previous chapter">Introducing Snowflake</a></li>
      <li>Next: <a href="CSVIngestion.html" title="next chapter">CSV ingestion</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" /> <br><br> <img src="images/frontpage.JPG" >
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>

  </body>
</html>