
<!DOCTYPE html>

<html>
  <head><link rel="shortcut icon" type="image/x-icon" href="SnowflakeEnriched.ico" />
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Introducing data platform &#8212; Contents v1.0 documentation</title>
    <link rel="stylesheet" href="static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="static/my.css" type="text/css" />
    <link rel="stylesheet" href="static/copybutton.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="static/documentation_options.js"></script>
    <script src="static/jquery.js"></script>
    <script src="static/underscore.js"></script>
    <script src="static/doctools.js"></script>
    <script src="static/language_data.js"></script>
    <script src="static/clipboard.min.js"></script>
    <script src="static/copybutton.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="CSV ingestion" href="CSVIngestion.html" />
    <link rel="prev" title="Introducing Snowflake" href="IntroducingSnowflake.html" />
   
  <link rel="stylesheet" href="static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="introducing-data-platform">
<h1>Introducing data platform<a class="headerlink" href="#introducing-data-platform" title="Permalink to this headline">¶</a></h1>
<p>The modern data architecture allows enterprises to ingest data coming from
multiple systems, in a variety of data formats, at different speeds, and at unknown
intervals. The layered data platform design makes it easy to process big
data efficiently. It enables organizations to quickly deploy new analytical
business-driven solutions to drive revenue and
profitability.</p>
<div class="section" id="pillars-of-data-platform">
<h2>Pillars of data platform<a class="headerlink" href="#pillars-of-data-platform" title="Permalink to this headline">¶</a></h2>
<p>There are four main pillars of the modern data platform:</p>
<ul class="simple">
<li><p>Data source layer</p></li>
<li><p>Data processing and storage layer</p></li>
<li><p>Analytics layer</p></li>
<li><p>Consumption layer</p></li>
</ul>
<div class="figure align-center">
<img alt="images/DataPlatform.jpg" src="images/DataPlatform.jpg" />
</div>
<div class="section" id="data-source-layer">
<h3>Data source layer<a class="headerlink" href="#data-source-layer" title="Permalink to this headline">¶</a></h3>
<p>Data source layer receives data from various sources.
Each source can be either internal or external to an organization
and generates data in real-time or in batch mode.
The <strong>variety</strong> of data formats can be structured, semi-structured, or unstructured.
The <strong>velocity</strong> (speed of arrival) and <strong>volume</strong> (delivery amount) of data will differ by source.</p>
</div>
<div class="section" id="data-processing-and-storage-layer">
<h3>Data processing and storage layer<a class="headerlink" href="#data-processing-and-storage-layer" title="Permalink to this headline">¶</a></h3>
<p>The data processing layer receives data from the data sources,
converts and stores the data into comprehensible format and ready to consume in servicing and analytical layer.
For example, large amounts of data are
stored in the Hadoop distributed file system store (HDFS). Large data
processing is performed through Hadoop/Spark systems. Data may
undergo format changes as it is processed through these systems. Cloud
service providers like Amazon, Google, and Microsoft allows to build and
operate data-centric applications on an infinite scale. Robust and
inexpensive storage is fundamental to the operation and scalability of the
big data architecture.</p>
<p>BigQuery, Azure Synapse, Amazon Redshift, and Snowflake
are used as standalone solutions for big data processing, or in
combination with the Hadoop/Spark ecosystems.</p>
</div>
<div class="section" id="analytical-layer">
<h3>Analytical layer<a class="headerlink" href="#analytical-layer" title="Permalink to this headline">¶</a></h3>
<p>The Analytical layer reads the data ingested and transformed by the data
processing and storage layer of the big data ecosystem. This layer consists of a variety of data
for different user requirements and
provides data discovery mechanisms on large volumes of
data. Apache Spark SQL, Hive, Apache Spark Streaming, Apache Spark GraphX, machine learning
libraries, SQL libraries, and a number of other toolsets
are utilized in this layer to understand the underlying data landscape.</p>
</div>
<div class="section" id="consumption-layer">
<h3>Consumption layer<a class="headerlink" href="#consumption-layer" title="Permalink to this headline">¶</a></h3>
<p>The consumption layer is also called the business intelligence layer. This layer
receives results from the analytical layer and presents the results
using visualization tools and
business processes.</p>
</div>
</div>
<div class="section" id="snowflake-data-pillars">
<h2>Snowflake data pillars<a class="headerlink" href="#snowflake-data-pillars" title="Permalink to this headline">¶</a></h2>
<p>For the project in this book, we will be using Snowflake staged files, transient tables, and master tables.
Transient tables are referenced as operational data tables throughout the book.
Master tables are also called as the servicing layer entities.
Snowflake staged files, transient tables along with the source system
represent data source layer.
Stored procedures are used for data processing.
Google Colab is used for the consumption layer.</p>
<p>The project work in this book, we seek to:</p>
<ul class="simple">
<li><p>Ingest variety of data formats.</p></li>
<li><p>Transform data for business users.</p></li>
<li><p>Build a data pipeline to process data on a periodic basis.</p></li>
<li><p>Create descriptive analytics for use-cases laid out by businesses.</p></li>
</ul>
<div class="section" id="staged-file">
<h3>Staged file<a class="headerlink" href="#staged-file" title="Permalink to this headline">¶</a></h3>
<p>All the data from the source system will be brought into the internal staged file location of Snowflake.
This data is directly sourced from transactional systems or existing big data platforms,
without applying any transformations.</p>
</div>
<div class="section" id="operational-datastore-od">
<h3>Operational datastore (OD)<a class="headerlink" href="#operational-datastore-od" title="Permalink to this headline">¶</a></h3>
<p>An operational datastore (OD) is a central store that provides a snapshot of
the latest data from multiple transactional systems.
It enables organizations to combine data in its original format
from various sources into a single destination to make it available for further processing.
These are initial physical tables created in Snowflake and consists of data
that has undergone little to no transformation.
The OD table structure very closely resembles that of the staged file with proper data types and column names, alongside
additional columns to help track data lineage. The OD also serves as transient location to store data coming from
the source systems. It allows us to have all the data within the bounds ready for processing and modeling.</p>
</div>
<div class="section" id="servicing-layer">
<h3>Servicing layer<a class="headerlink" href="#servicing-layer" title="Permalink to this headline">¶</a></h3>
<p>This layer consists of the data that has undergone all sorts of transformations and it is
ready for consumption to the business intelligence layer.
OD tables serve as input to the transformation process to populate servicing layer tables.
This is our master schema with transformed data. It contains correctly modeled tables,
that are named properly with business centric column names with appropriate data types.</p>
</div>
<div class="section" id="dataflow-summary">
<h3>Dataflow summary<a class="headerlink" href="#dataflow-summary" title="Permalink to this headline">¶</a></h3>
<p>The data cleaning and transformation takes place while moving data from OD into service layer.</p>
<ul class="simple">
<li><p>Standardizing all date formats and time zones.</p></li>
<li><p>Rounding numbers.</p></li>
<li><p>Cleaning strings and character data.</p></li>
<li><p>Synthesizing data to be of the same format.</p></li>
<li><p>Splitting out data into multiple columns.</p></li>
<li><p>Extracting data from CSV, JSON, ORC, and Parquet.</p></li>
<li><p>Combining data from multiple columns to Single JSON objects.</p></li>
</ul>
<div class="figure align-center">

</div>
<p>The diagram below shows the data flow process between files and the final destination in Snowflake.</p>
<div class="figure align-center">
<img alt="images/ingestionlayout.jpg" src="images/ingestionlayout.jpg" />
</div>
<p>All the data transformation is done though the stored procedures (SP). In this book all the SPs follows these common pattern.</p>
<blockquote>
<div><ul class="simple">
<li><p>The procedure is defined with argument.</p></li>
<li><p>JavaScript is the runtime language.</p></li>
<li><p>The Procedure returns a string success or failure.</p></li>
<li><p>To execute single SQL statment, SQL script is assigned to <strong>JavaScript</strong> variable and <strong>snowflake.execute</strong>  is called by passing this JavaScript variable.</p></li>
<li><p>For mulitple SQL statments, different JavaScript variables are used.</p></li>
<li><p>These variables are combined into a string array.</p></li>
<li><p>For loop is used to execute all SQL commands by calling snowflake.execute and passing the JavaScript variable.</p></li>
<li><p>Results are checked on each call of snowflake.execute.</p></li>
</ul>
</div></blockquote>
<div class="figure align-center">

</div>
<p>Not all data from staged files and OD tables may land up in the servicing layer.</p>
<p>Data engineering create the business logic-based transformations of transient data store. They are
responsible for metadata management and keeping track of data pipelines.</p>
<p>Normally, business users like to use tools such as PowerBI and Tableau to perform descriptive analytics
and build reports, dashboards. The service layer is the best place for business users to get their data needs.</p>
<p>And finally, data scientist like to perform experiments with raw and transformed data combined.
They may use the staged and OD attributes along with servicing layer to build predictive analytics.</p>
</div>
</div>
<div class="section" id="project-data-source">
<h2>Project data source<a class="headerlink" href="#project-data-source" title="Permalink to this headline">¶</a></h2>
<p>This book is designed to use data from Integrated Postsecondary Education Data System (IPEDS)
and build analytics ecosystem using this data. IPEDS  <a class="reference external" href="https://nces.ed.gov/ipeds"  target="_blank">https://nces.ed.gov/ipeds</a>  is
established as the core postsecondary education data collection program for NCES.
It is the primary source for information on US Colleges,
universities, technical institutions, and vocational institutions.</p>
<p>The National Center for Education Statistics (NCES) is the primary federal entity for collecting
and analyzing data related to education in the US and other nations.
NCES is located within the US Department of Education and the Institute of Education Sciences.
NCES fulfills a Congressional mandate to collect, collate, analyze, and report complete statistics
on the condition of American education; conduct and publish reports; and review and report on
education activities internationally.</p>
<div class="figure align-center">

</div>
<p>All of the data used in this book is downloaded from <a class="reference external" href="https://nces.ed.gov/ipeds/use-the-data"   target="_blank">https://nces.ed.gov/ipeds/use-the-data</a>.</p>
<p>From IPEDS, we are downloading below listed files for 2017, 2018, and 2019 academic years.</p>
<p><strong>Academic institution [HDR]</strong></p>
<p>These files contain directory information for every institution in the IPEDS universe and contain data attributes
like name, address, city, state, zip code, and various URL links to the institution's home page,
admissions, and financial aid offices, and net price calculator.</p>
<p><strong>Enrollment 12-Month [EFFY]</strong></p>
<p>These files contain the unduplicated headcount of students enrolled over a
12-month period for both undergraduate and graduate levels. Each record is
uniquely defined by the variables IPEDS, ID, and the level of enrollment.</p>
<p><strong>Institutional charges [IC_AY]</strong></p>
<p>These files contain data on student charges for a full academic year. The cost of attendance includes the
published tuition and required fees, books and supplies, room and board, and other expenses.</p>
<p><strong>Admissions and test scores [ADM]</strong></p>
<p>These files contain information about the undergraduate selection process
for first-time, degree/certificate-seeking prospective students.</p>
<p><strong>Code mapping data</strong></p>
<p>These files contain data for lookup tables.
A lookup table acts as a dictionary. It provides descriptions for each business codes.
For example: Institute type description for the given institute type code,
State name for the state code, etc..</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><em>Disclaimer: We have modified this data to explore Snowflake functionality. Therefore, analytics results from this book
should not be used as a replacement for the original data and its results.
Please refer to the NCES website for actual statistics and research on academic institutions.</em></p>
<p><em>Utilities for converting the CSV files into different data formats are available at:</em>
<a class="reference external" href="https://github.com/versatiledp/SnowflakeEnriched/tree/master/utilities"   target="_blank">https://github.com/versatiledp/SnowflakeEnriched/tree/master/utilities</a></p>
<p><em>All the data files used throughout this book are available to download at:</em>
<a class="reference external" href="https://github.com/versatiledp/SnowflakeEnriched/tree/master/data/input"  target="_blank">https://github.com/versatiledp/SnowflakeEnriched/tree/master/data/input</a></p>
</div>
<p>These files are available to process in various formats like CSV, JSON, ORC, Parquet, and Multi-format.</p>
<div class="section" id="academic-institution-hdr">
<h3>Academic institution [HDR]<a class="headerlink" href="#academic-institution-hdr" title="Permalink to this headline">¶</a></h3>
<p>Academic institution files are available in CSV format at the source system.
CSV is a simple basic comma-separated file format.
Each line of tabular data is saved as a record.
Record consist of multiple fields separated by commas.
CSV files are directly ingested into Snowflake staging from the source system.
The following diagram displays
data flow between the source system and the Snowflake destination.</p>
<div class="figure align-center">
<img alt="images/csvIngestion.jpg" src="images/csvIngestion.jpg" />
</div>
<p>ELT layer defines the schema for the CSV file with data types.
The OD layer is used to process data for the destination layer with additional transformations.</p>
</div>
<div class="section" id="enrollment-12-month-effy">
<h3>Enrollment 12-Month [EFFY]<a class="headerlink" href="#enrollment-12-month-effy" title="Permalink to this headline">¶</a></h3>
<p>EFFY files received from IPEDS are in CSV format. To simulate JSON data, we used
Python code <strong>csvToJson.py</strong> to convert CSV files to JSON format.
JSON is a common open standard file and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute-value pairs and array data types. This data format is used in a diverse range of applications and serves as a replacement for XML in AJAX systems.
In our use-case, we are ingesting enrollment (EFFY) files in the JSON format directly to Snowflake.
The following diagram displays the data ingestion architecture for JSON files.</p>
<div class="figure align-center">
<img alt="images/jsonIngestion.jpg" src="images/jsonIngestion.jpg" />
</div>
<p>We convert JSON data into relational table structure in the Snowflake transformation.</p>
</div>
<div class="section" id="institutional-charges-ic">
<h3>Institutional charges [IC]<a class="headerlink" href="#institutional-charges-ic" title="Permalink to this headline">¶</a></h3>
<p>Institutional charges files are available in CSV format at the source system.
To create ORC file for simulation in this book, Jupyter notebook <strong>csvToOrc.ipynb</strong> is used.
This Python code migrated CSV files from IPEDS universe to ORC format.
The Optimized Row Columnar (ORC) file format provides a highly efficient way to store Hive data. ORC files improve performance when Hive is reading, writing, and processing data.
The compact nature of this format is ideal, and enables skipping over irrelevant parts without needing large, complex, or manually maintained indices.</p>
<p>The following diagram displays the data ingestion architecture for Hadoop-based big data platform systems generating
data in ORC format.</p>
<div class="figure align-center">
<img alt="images/orcIngestion.jpg" src="images/orcIngestion.jpg" />
</div>
<p>Data from different source systems are brought to Hadoop environment for big data processing. We would use ORC files
coming out of Hadoop and ingest into Snowflake.</p>
<p>ORC data is converted into relational table structure in the Snowflake transformation.
OD layer is used to process data for the destination layer with additional transformation.</p>
</div>
<div class="section" id="admissions-and-test-scores-adm">
<h3>Admissions and test scores [ADM]<a class="headerlink" href="#admissions-and-test-scores-adm" title="Permalink to this headline">¶</a></h3>
<p>Similar to previous sections, these files are available in CSV format.
For simulating Parquet data, we have utilized <strong>csvToParquet.ipynb</strong> notebook.
Parquet is an open-source file format for Hadoop and Spark.
It stores nested data structures in a flat columnar format.
Compared to a traditional approach where data is stored in a row-oriented approach, Parquet is more efficient
in terms of storage and performance. It is especially useful for queries that read specific columns
from a wide table. Parquet provides optimizations to speed up queries.
Apache Spark has become the de-facto standard for big data processing recently. Spark output is written in
Parquet format.
The following diagram lays out the architecture to ingest Parquet data from Apache Spark based big data platform.</p>
<div class="figure align-center">
<img alt="images/parquetIngestion.jpg" src="images/parquetIngestion.jpg" />
</div>
<p>We convert Parquet data into relational table structure in the Snowflake transformation.
OD layer is used to process data for the destination layer with additional transformation.</p>
</div>
<div class="section" id="code-mapping-data">
<h3>Code mapping data<a class="headerlink" href="#code-mapping-data" title="Permalink to this headline">¶</a></h3>
<p>This Multi-format dataset has a special character delimiter. One file holds data for multiple destination tables, which are
identified by the value of the first column in the file.</p>
<p>These files are directly ingested into Snowflake staging from the source system.
Multiple lookup OD tables will be populated from one source file.</p>
<p>Download all the converted source files from GitRepo to local machine.
Create folder <strong>C:\Snowflake\data\input\</strong> and copy downloaded files to this folder.
The files in this book are loaded from the above folder to the Snowflake stage area using SnowSQL CLI.</p>
<p>GitRepo: <a class="reference external" href="https://github.com/versatiledp/SnowflakeEnriched/tree/master/data/input"  target="_blank">https://github.com/versatiledp/SnowflakeEnriched/tree/master/data/input</a></p>
</div>
</div>
<div class="section" id="ingesting-files-to-snowflake-stage">
<h2>Ingesting files to Snowflake stage<a class="headerlink" href="#ingesting-files-to-snowflake-stage" title="Permalink to this headline">¶</a></h2>
<p>Data files are received in various formats like CSV, ORC, Parquet, and JSON. These files are available in the folders by year. They are brought into Snowflake staged files using the PUT command.
Each staged file contains the entire data for that specific year. From these staged files, OD tables are populated.
Using these OD tables, servicing layer tables of the data-warehouse are populated. Snowflake's stored procedures, views, and functions are used for ETL/ELT processes.</p>
<p>Internal named stage needs to be created for each of the file formats CSV, ORC, Parquet, and JSON before loading data.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="line-block">
<div class="line"><em>All the Snowflake SQL commands in this book are to be executed from Snowflake worksheet unless specified differently</em>.</div>
<div class="line"><br /></div>
<div class="line"><em>Login to the Snowflake account</em>.</div>
<div class="line"><em>Use IPEDS database</em>.</div>
<div class="line"><em>Copy the code from book and execute</em>.</div>
</div>
</div>
<div class="section" id="create-internal-stage-area-ipeds">
<h3>Create internal stage area IPEDS<a class="headerlink" href="#create-internal-stage-area-ipeds" title="Permalink to this headline">¶</a></h3>
<p>First, create the file formats and stage area for the following data formats:</p>
<blockquote>
<div><ul class="simple">
<li><p>Tab delimited</p></li>
<li><p>Comma separated with a header record</p></li>
<li><p>Comma separated with no header</p></li>
</ul>
</div></blockquote>
<div class="figure align-center">

</div>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">--</span> <span class="n">Number</span> <span class="n">of</span> <span class="n">header</span> <span class="n">lines</span> <span class="n">at</span> <span class="n">the</span> <span class="n">start</span> <span class="n">of</span> <span class="n">the</span> <span class="n">file</span><span class="o">.</span>
<span class="o">--</span> <span class="n">The</span> <span class="n">COPY</span> <span class="n">command</span> <span class="n">skips</span> <span class="n">header</span> <span class="n">lines</span> <span class="n">when</span> <span class="n">loading</span> <span class="n">data</span>
<span class="o">--</span> <span class="n">All</span> <span class="n">these</span> <span class="n">file</span> <span class="n">formats</span> <span class="n">can</span> <span class="n">be</span> <span class="n">used</span> <span class="n">to</span> <span class="n">create</span> <span class="n">different</span> <span class="n">stage</span> <span class="n">areas</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span>  <span class="n">FILE</span> <span class="n">FORMAT</span>
                <span class="n">ff_IPEDS_CSVSkipHeaderTabDelimited</span>
                        <span class="n">TYPE</span> <span class="o">=</span>   <span class="n">CSV</span>
                        <span class="n">FIELD_DELIMITER</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span>  <span class="n">COMPRESSION</span> <span class="o">=</span> <span class="n">AUTO</span>
                        <span class="n">SKIP_HEADER</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span>  <span class="n">FILE</span> <span class="n">FORMAT</span>
                <span class="n">ff_IPEDS_CSVSkipHeaderCommaDelimited</span>
                        <span class="n">TYPE</span> <span class="o">=</span>   <span class="n">CSV</span>
                        <span class="n">FIELD_DELIMITER</span> <span class="o">=</span> <span class="s1">&#39;,&#39;</span>  <span class="n">COMPRESSION</span> <span class="o">=</span> <span class="n">AUTO</span>
                        <span class="n">SKIP_HEADER</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="o">--</span> <span class="n">The</span> <span class="n">COPY</span> <span class="n">command</span> <span class="n">does</span> <span class="ow">not</span> <span class="n">skip</span> <span class="nb">any</span> <span class="n">lines</span><span class="o">.</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span>  <span class="n">FILE</span> <span class="n">FORMAT</span>
                <span class="n">ff_IPEDS_CSVHeaderTabDelimited</span>
                        <span class="n">TYPE</span> <span class="o">=</span>   <span class="n">CSV</span>
                        <span class="n">FIELD_DELIMITER</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span>  <span class="n">COMPRESSION</span> <span class="o">=</span> <span class="n">AUTO</span>
                        <span class="n">SKIP_HEADER</span> <span class="o">=</span> <span class="mi">0</span> <span class="p">;</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span>  <span class="n">FILE</span> <span class="n">FORMAT</span>
                <span class="n">ff_IPEDS_CSVHeaderCommaDelimited</span>
                        <span class="n">TYPE</span> <span class="o">=</span>   <span class="n">CSV</span>
                        <span class="n">FIELD_DELIMITER</span> <span class="o">=</span> <span class="s1">&#39;,&#39;</span>  <span class="n">COMPRESSION</span> <span class="o">=</span> <span class="n">AUTO</span>
                        <span class="n">SKIP_HEADER</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">encoding</span> <span class="o">=</span> <span class="s1">&#39;iso-8859-1&#39;</span>
                        <span class="n">FIELD_OPTIONALLY_ENCLOSED_BY</span><span class="o">=</span><span class="s1">&#39;&quot;&#39;</span><span class="p">;</span>
<span class="o">--</span> <span class="n">create</span> <span class="n">satge</span> <span class="n">area</span> <span class="n">to</span> <span class="n">load</span> <span class="n">csv</span> <span class="n">files</span> <span class="k">for</span> <span class="n">the</span> <span class="n">project</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span> <span class="n">STAGE</span> <span class="n">IPEDS_HD</span> <span class="n">FILE_FORMAT</span>
                        <span class="o">=</span> <span class="n">ff_IPEDS_CSVHeaderCommaDelimited</span><span class="p">;</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span> <span class="n">STAGE</span> <span class="n">IPEDS_CM</span> <span class="n">FILE_FORMAT</span>
                        <span class="o">=</span> <span class="n">ff_IPEDS_CSVSkipHeaderTabDelimited</span><span class="p">;</span>
</pre></div>
</div>
<p>Next, create the JSON file format and stage area.</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">--</span> <span class="n">Create</span> <span class="n">file</span> <span class="nb">format</span> <span class="n">to</span> <span class="n">load</span> <span class="n">JSON</span> <span class="n">file</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span> <span class="n">FILE</span> <span class="n">FORMAT</span> <span class="n">ff_IPEDS_Json</span>
                <span class="n">TYPE</span> <span class="o">=</span><span class="n">JSON</span> <span class="n">TRIM_SPACE</span> <span class="o">=</span> <span class="n">TRUE</span><span class="p">;</span>
<span class="o">--</span> <span class="n">Create</span> <span class="n">stage</span> <span class="n">area</span> <span class="n">to</span> <span class="n">load</span> <span class="n">JSON</span> <span class="n">file</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span> <span class="n">stage</span> <span class="n">IPEDS_EFFY</span> <span class="n">FILE_FORMAT</span> <span class="o">=</span> <span class="n">ff_IPEDS_Json</span><span class="p">;</span>
</pre></div>
</div>
<p>Then, create the ORC file format and stage area.</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">--</span> <span class="n">Create</span> <span class="n">file</span> <span class="nb">format</span> <span class="n">to</span> <span class="n">load</span> <span class="n">ORC</span> <span class="n">file</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span> <span class="n">FILE</span> <span class="n">FORMAT</span> <span class="n">ff_IPEDS_ORC</span>
                        <span class="n">TYPE</span> <span class="o">=</span><span class="n">ORC</span> <span class="n">TRIM_SPACE</span> <span class="o">=</span> <span class="n">TRUE</span><span class="p">;</span>
<span class="o">--</span> <span class="n">Create</span> <span class="n">stage</span> <span class="n">area</span> <span class="n">to</span> <span class="n">load</span> <span class="n">ORC</span> <span class="n">file</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span> <span class="n">stage</span> <span class="n">IPEDS_IC</span> <span class="n">FILE_FORMAT</span> <span class="o">=</span> <span class="n">ff_IPEDS_ORC</span><span class="p">;</span>
</pre></div>
</div>
<p>Finally, create the Parquet file format and stage area.</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">--</span> <span class="n">Create</span> <span class="n">Parquet</span> <span class="n">file</span> <span class="nb">format</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span>  <span class="n">FILE</span> <span class="n">FORMAT</span> <span class="n">ff_IPEDS_Parquet</span>
                        <span class="n">TYPE</span> <span class="o">=</span><span class="n">PARQUET</span> <span class="n">TRIM_SPACE</span> <span class="o">=</span> <span class="n">TRUE</span><span class="p">;</span>
<span class="o">--</span> <span class="n">Create</span> <span class="n">stage</span> <span class="n">area</span> <span class="n">to</span> <span class="n">load</span> <span class="n">Parquet</span> <span class="n">file</span>
<span class="n">CREATE</span> <span class="n">OR</span> <span class="n">REPLACE</span> <span class="n">stage</span> <span class="n">IPEDS_ADM</span> <span class="n">FILE_FORMAT</span> <span class="o">=</span> <span class="n">ff_IPEDS_Parquet</span><span class="p">;</span>
</pre></div>
</div>
<p>Once these formats and stage areas are created,
we can validate their availability.</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">SHOW</span> <span class="n">STAGES</span> <span class="p">;</span>
<span class="n">SHOW</span> <span class="n">STAGES</span> <span class="n">LIKE</span> <span class="s1">&#39;%IPED%&#39;</span><span class="p">;</span>
</pre></div>
</div>
<div class="figure align-center">
<img alt="images/stagefilelist.JPG" src="images/stagefilelist.JPG" />
</div>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">SHOW</span> <span class="n">FILE</span> <span class="n">FORMATS</span><span class="p">;</span>
<span class="n">SHOW</span> <span class="n">FILE</span> <span class="n">FORMATS</span> <span class="n">LIKE</span> <span class="s1">&#39;%IPED%&#39;</span><span class="p">;</span>
</pre></div>
</div>
<div class="figure align-center">
<img alt="images/formats.JPG" src="images/formats.JPG" />
</div>
<p>At this point, we have created an internal named stage in the Snowflake to ingest CSV, Parquet, ORC, and JSON files.</p>
<p>We will use the following stage area to load the raw files.</p>
<blockquote>
<div><ul class="simple">
<li><p>IPEDS_ADM</p></li>
<li><p>IPEDS_EFFY</p></li>
<li><p>IPEDS_CM</p></li>
<li><p>IPEDS_HD</p></li>
<li><p>IPEDS_IC</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="load-the-files-to-internal-stage-for-ipeds">
<h3>Load the files to internal stage for IPEDS<a class="headerlink" href="#load-the-files-to-internal-stage-for-ipeds" title="Permalink to this headline">¶</a></h3>
<p>In this section, we are ingesting all the files to the Snowflake staging area. We assume you have
downloaded the files from GitRepo and copied to the folder C:\Snowflake\data\input\.</p>
<blockquote>
<div><ul class="simple">
<li><p>Right click on command prompt application in windows machine and run as administrator.</p></li>
<li><p>Change directory by executing CD C:\Snowflake\data\input from the command prompt.</p></li>
<li><p>Check content of the folder with DIR command.</p></li>
</ul>
</div></blockquote>
<div class="figure align-center">
<img alt="images/inputfileList.JPG" src="images/inputfileList.JPG" />
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="line-block">
<div class="line"><em>Connect to the Snowflake database using SnowSQL CLI</em>.</div>
<div class="line"><em>Execute all the commands listed below in this section from SnowSQL CLI</em>.</div>
</div>
</div>
<p>Load the CSV academic institution (HD) files.</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">HD2017</span><span class="o">.</span><span class="n">csv</span> <span class="nd">@IPEDS_HD</span><span class="p">;</span>
<span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">HD2018</span><span class="o">.</span><span class="n">csv</span> <span class="nd">@IPEDS_HD</span><span class="p">;</span>
<span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">HD2019</span><span class="o">.</span><span class="n">csv</span> <span class="nd">@IPEDS_HD</span><span class="p">;</span>
</pre></div>
</div>
<p>Load the JSON enrollment (EFFY) files.</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">effy2017_rv</span><span class="o">.</span><span class="n">json</span> <span class="nd">@IPEDS_EFFY</span><span class="p">;</span>
<span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">effy2018_rv</span><span class="o">.</span><span class="n">json</span> <span class="nd">@IPEDS_EFFY</span><span class="p">;</span>
<span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">effy2019_rv</span><span class="o">.</span><span class="n">json</span> <span class="nd">@IPEDS_EFFY</span><span class="p">;</span>
</pre></div>
</div>
<p>Load the ORC institutional charges (IC)  files.
All the ORC partitions are loaded into a specific folder of ORC staging area.</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">ic2017_ay_orc</span>\<span class="o">*.*</span> <span class="nd">@IPEDS_IC</span><span class="o">/</span><span class="mi">2017</span><span class="o">/</span><span class="p">;</span>
<span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">ic2018_ay_orc</span>\<span class="o">*.*</span> <span class="nd">@IPEDS_IC</span><span class="o">/</span><span class="mi">2018</span><span class="o">/</span><span class="p">;</span>
<span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">ic2019_ay_orc</span>\<span class="o">*.*</span> <span class="nd">@IPEDS_IC</span><span class="o">/</span><span class="mi">2019</span><span class="o">/</span><span class="p">;</span>
</pre></div>
</div>
<p>Load the Parquet admission (ADM) files.
All the Parquet partitions are loaded into a specific folder of the Parquet staging area.</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">adm2017</span><span class="o">.</span><span class="n">parquet</span>\<span class="o">*.*</span> <span class="nd">@IPEDS_ADM</span><span class="o">/</span><span class="mi">2017</span><span class="o">/</span><span class="p">;</span>
<span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">adm2018</span><span class="o">.</span><span class="n">parquet</span>\<span class="o">*.*</span> <span class="nd">@IPEDS_ADM</span><span class="o">/</span><span class="mi">2018</span><span class="o">/</span><span class="p">;</span>
<span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">adm2019</span><span class="o">.</span><span class="n">parquet</span>\<span class="o">*.*</span> <span class="nd">@IPEDS_ADM</span><span class="o">/</span><span class="mi">2019</span><span class="o">/</span><span class="p">;</span>
</pre></div>
</div>
<p>Load the CSV code mapping (CM) files.</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">put</span> <span class="n">file</span><span class="p">:</span><span class="o">//</span><span class="n">C</span><span class="p">:</span>\<span class="n">snowflake</span>\<span class="n">data</span>\<span class="nb">input</span>\<span class="n">CodeMappingData</span><span class="o">.</span><span class="n">txt</span> <span class="nd">@IPEDS_CM</span><span class="p">;</span>
</pre></div>
</div>
<p>Once these files are loaded to the stage area,
we can check their availability in the internal stage area by executing the code from Snowflake worksheet area as follows:</p>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span> <span class="nd">@IPEDS_HD</span><span class="p">;</span>
</pre></div>
</div>
<div class="figure align-center">
<img alt="images/csvlist.JPG" src="images/csvlist.JPG" />
</div>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span> <span class="nd">@IPEDS_EFFY</span><span class="p">;</span>
</pre></div>
</div>
<div class="figure align-center">
<img alt="images/jsonlist.JPG" src="images/jsonlist.JPG" />
</div>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span> <span class="nd">@IPEDS_IC</span><span class="p">;</span>
</pre></div>
</div>
<div class="figure align-center">
<img alt="images/stagedORC.JPG" src="images/stagedORC.JPG" />
</div>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span> <span class="nd">@IPEDS_ADM</span><span class="p">;</span>
</pre></div>
</div>
<div class="figure align-center">
<img alt="images/Parquetlist.JPG" src="images/Parquetlist.JPG" />
</div>
<div class="code highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span> <span class="nd">@IPEDS_CM</span><span class="p">;</span>
</pre></div>
</div>
<div class="figure align-center">
<img alt="images/csvOther.JPG" src="images/csvOther.JPG" />
</div>
<div class="figure align-center">

</div>
<p>Terminate the current session as shown below.</p>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>!exit
</pre></div>
</div>
<p>All the files from the source system are ingested into the Snowflake staged area.
If there are any issues in the staged area, drop staged area using <strong>remove &#64;StageAreaName</strong>.
Example to drop ADM file stage area, execute the following command.</p>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>remove @IPEDS_ADM
</pre></div>
</div>
<p>In order to recreate dropped staged area, re-run scripts for
creating and loading the staged area.</p>
</div>
</div>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>This chapter summarizes, data files, file formats, and processes used to simplify
and streamline bulk loading data into various
internal staging areas.
The data from the transactional source systems go through multiple data processing layers.
This results in a variety of data formats being created.
We map these data files into the analytical metadata layer. This chapter
summarized all the data formats we will be dealing with in the processing layer of the analytical data platform.</p>
<p>With this understanding of Snowflake and data platform overview, let us start ingesting
our first set of CSV files from the source system.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><em>SourceCode:</em>
<a class="reference external" href="https://github.com/versatiledp/SnowflakeEnriched/tree/master/source/code/SQL/StagedFile">https://versatiledp.github.io/SnowflakeEnriched/tree/master/source/code/SQL/StagedFile</a></p>
<div class="line-block">
<div class="line"><em>The Mapping between the source system and Snowflake metadata is at:</em></div>
<div class="line"><a class="reference external" href="https://versatiledp.github.io/SnowflakeEnriched/Mapping/AcademicInstitute.htm">https://versatiledp.github.io/SnowflakeEnriched/Mapping/AcademicInstitute</a></div>
<div class="line"><a class="reference external" href="https://versatiledp.github.io/SnowflakeEnriched/Mapping/Enrollment.htm">https://versatiledp.github.io/SnowflakeEnriched/Mapping/Enrollment</a></div>
<div class="line"><a class="reference external" href="https://versatiledp.github.io/SnowflakeEnriched/Mapping/InstitutionCharges.htm">https://versatiledp.github.io/SnowflakeEnriched/Mapping/InstitutionCharges</a></div>
<div class="line"><a class="reference external" href="https://versatiledp.github.io/SnowflakeEnriched/Mapping/AdmissionStat.htm">https://versatiledp.github.io/SnowflakeEnriched/Mapping/AdmissionStat</a></div>
<div class="line"><a class="reference external" href="https://versatiledp.github.io/SnowflakeEnriched/Mapping/CodeValue.htm">https://versatiledp.github.io/SnowflakeEnriched/Mapping/CodeValue</a></div>
</div>
</div>
<table class="docutils align-default">
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="./index.html">&nbsp;&nbsp;<img src= "images/tableofcontents.png" width="25px" height="20px"><font color="blue"><b>Table of contents</b> </font></a></p></td>
<td><p><a class="reference external" href="./IntroducingSnowflake.html">&nbsp;&nbsp;<img src= "images/previouschapter.png" width="30px" height="20px"><font color="blue"><b>Previous chapter</b> </font></a></p></td>
<td><p><a class="reference external" href="./CSVIngestion.html">&nbsp;&nbsp;<img src= "images/nextchapter.png" width="30px" height="20px"><font color="blue"><b>Next chapter</b> </font></a></p></td>
</tr>
</tbody>
</table>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Contents</a></h1>









<p class="caption"></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Preface.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="IntroducingSnowflake.html">Introducing Snowflake</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Introducing data platform</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#pillars-of-data-platform">Pillars of data platform</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#data-source-layer">Data source layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-processing-and-storage-layer">Data processing and storage layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#analytical-layer">Analytical layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#consumption-layer">Consumption layer</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#snowflake-data-pillars">Snowflake data pillars</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#staged-file">Staged file</a></li>
<li class="toctree-l3"><a class="reference internal" href="#operational-datastore-od">Operational datastore (OD)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#servicing-layer">Servicing layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dataflow-summary">Dataflow summary</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#project-data-source">Project data source</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#academic-institution-hdr">Academic institution [HDR]</a></li>
<li class="toctree-l3"><a class="reference internal" href="#enrollment-12-month-effy">Enrollment 12-Month [EFFY]</a></li>
<li class="toctree-l3"><a class="reference internal" href="#institutional-charges-ic">Institutional charges [IC]</a></li>
<li class="toctree-l3"><a class="reference internal" href="#admissions-and-test-scores-adm">Admissions and test scores [ADM]</a></li>
<li class="toctree-l3"><a class="reference internal" href="#code-mapping-data">Code mapping data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#ingesting-files-to-snowflake-stage">Ingesting files to Snowflake stage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#create-internal-stage-area-ipeds">Create internal stage area IPEDS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#load-the-files-to-internal-stage-for-ipeds">Load the files to internal stage for IPEDS</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="CSVIngestion.html">CSV ingestion</a></li>
<li class="toctree-l1"><a class="reference internal" href="JSONIngestion.html">JSON ingestion</a></li>
<li class="toctree-l1"><a class="reference internal" href="ORCIngestion.html">ORC ingestion</a></li>
<li class="toctree-l1"><a class="reference internal" href="ParquetIngestion.html">Parquet ingestion</a></li>
<li class="toctree-l1"><a class="reference internal" href="SemiStructuredDataIngestion.html">Multi-format ingestion</a></li>
<li class="toctree-l1"><a class="reference internal" href="PipelineOrchestration.html">Workflow orchestration</a></li>
<li class="toctree-l1"><a class="reference internal" href="BusinessInsights.html">Business insights</a></li>
<li class="toctree-l1"><a class="reference internal" href="Summary.html">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="Abbreviations.html">Abbreviations</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="IntroducingSnowflake.html" title="previous chapter">Introducing Snowflake</a></li>
      <li>Next: <a href="CSVIngestion.html" title="next chapter">CSV ingestion</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" /> <br><br> <img src="images/frontpage.JPG" >
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>

  </body>
</html>