Introducing Snowflake
========================================

Snowflake is elastic data warehousing service.
It handles infrastructure, optimization, availability, and data protection.
It is a cloud-based SaaS (Software-as-a-Service) data management solution that makes it possible for 
organizations to:

-  Bring all their business data together in one place for analysis.
-  Allow all analysts, applications, or even customers to have shared access to the data.
-  Focus on analyzing data without worrying about hardware, software, or database tuning.
-  Utilize the SQL skills data, the community already has built.
-  Leverage the existing ecosystem of tools to manage, extract transform load (**ETL**) or extract load transform (**ELT**) process.
-  Integrate the current business intelligence stack like Tableau, PowerBI, and other open-source tools like D3.js and Plotly seamlessly.

Snowflake in a nutshell
----------------------------------

Snowflake data warehousing is built with cloud-first architecture. 
Snowflake core is built with separation of storage and compute. Because of this,
it allows infinite scalability for data storage and compute resources. 
This also enables organizations to execute data load pipelines and create analytical insights at the same time, 
without competing for resources. Snowflake also provides all the capabilities needed for 
enabling data lake, data exchange, data applications, and data engineering. 

Snowflake is a cloud-agnostic platform providing, unified data management across multiple cloud service providers like Azure, 
AWS, and GCP. In other words, the Snowflake data warehouse can be built on top of Amazon web services, Microsoft Azure 
cloud infrastructure, or Google cloud.
 

Architecture
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The Snowflake architecture consists of three layers.
The following diagram shows Snowflake architecture.

.. figure:: images/snowflakeArchitecture.JPG
   :align: center


Source: https://www.snowflake.com/blog/5-reasons-to-love-snowflakes-architecture-for-your-data-warehouse/


**Cloud Services:** This layer coordinates and handles all other services in Snowflake,
including sessions, authentication, SQL compilation, encryption, etc.

**Compute resources:** Snowflake provides the ability to create virtual warehouses,
which are basically compute clusters provisioned behind the scenes. 
These virtual warehouses are used to load data, run queries, or do both concurrently. 
 
**Storage resources:**  The actual underlying file system is backed by the Snowflake account, in which
all data is encrypted, compressed, and distributed to optimize performance. 
Snowflake also provides excellent data durability, and availability.



Virtual warehouse
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Snowflake defines a virtual warehouse as a cluster of compute resources. 
The warehouse provides all the resources like CPU, memory, and temporary storage 
required to start using Snowflake. This warehouse defaults to auto-suspend mode if there are no activities.

All data in Snowflake is maintained in databases. Each database consists of one or more schemas. 
Schemas are logical groupings of database objects like tables, views, and functions. 

Advantages
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

-  Faster, easier to use, scalable, and far more flexible than other data platforms.
-  Can be started quickly and delivers high-quality performance.
-  Solves concurrency issues with its multi-cluster warehouse architecture.
-  Loads and processes the data quickly.
-  Supports auto-scaling, big data workload, and data sharing.
-  Easily scale up the virtual warehouse and provides elasticity as per needs.
-  Intuitive and simple interface.
-  Fully automated platform.

Data features
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In Snowflake, data can be ingested from a variety of data 
formats like CSV, Parquet, ORC, and XML data sources. 
It supports JSON to a great extent, and furthermore:

-  Provides full support to create views on different data formats.
-  Allows securely sharing data with other snowflake accounts. 
-  Permits replication and syncing databases across multiple accounts in different regions. 
-  Enables time travel functions to query the history of data changes over a period of time.  


Data sharing
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Snowflake data sharing is a powerful and simple feature to share 
the data from one account to another account. 
The data producer can provide access to the live data without copying 
or moving the data. The data consumer can query the shared data from 
the producer without any performance bottlenecks because of multi-cluster shared data architecture.


Security
^^^^^^^^^^^^^^^^^^^^^^^^^^
Snowflake provides out-of-the-box network access control via network policies, 
allowing to restrict account access to specific IP addresses.
Snowflake offers Role Based Access Control. Users can be 
defined in the Snowflake as well as external identity providers 
such as Active Directory and Okta. External identities allow federation of user management. 
We can set the privileges to specific roles. These roles are assigned to the users.


Time travel
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The time travel feature allows querying the historical data of a table.
If we delete or update data rows, we can retrieve the status of the 
table at point in time before we executed that statement. 
The time travel is applied by default on all tables in Snowflake 
for 1 day and can be configured up to 90 days.


Connectivity
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Snowflake provides a few different methods to create database connectivity:  

-  ODBC and JDBC drivers can directly access a Snowflake instance. 
-  Web-based worksheets can be leveraged within the Snowflake account.
-  SnowSQL CLI is also used to establish connectivity to Snowflake.



Data load tools
----------------------------------------------------

Snowflake data load can be achieved using:

	-  Web interface to load a small amount of data.
	-  SnowSQL CLI for the bulk load.
	-  Snowpipe to automate the load of large data set across multiple platforms. 

Snowflake supports a wide range of third-party tools like Informatica, Talend, and Azure Data Factory.

Snowflake web
^^^^^^^^^^^^^^^^^^^^^^^

The Snowflake web interface provides a convenient wizard that allows a selection of specific tables. 
Using the LOAD button, we can easily ingest a limited amount of data. 
The wizard simplifies the load by combining the staging and data loading phases into a single 
operation. Also, it automatically deletes the staged file after loading.

SnowSQL CLI Client 
^^^^^^^^^^^^^^^^^^^^^^^^^^^

SnowSQL CLI is the command-line client interface for Snowflake. It executes SQL queries 
and performs all DDL and DML operations. It is an easy way to access Snowflake from the command line and has
the same capabilities as the Snowflake UI.

Snowpipe
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Snowpipe is a mechanism to load high frequency or streaming data.  
It can load data in a near real-time manner. It also allows for the processing of data in micro-batches. 
Micro-batch processing is very similar to traditional batch processing in that data is usually processed as a group.
Snowpipe is a serverless architecture
and does not use virtual warehouse resources. It has its own resources that are managed by Snowflake instances. 

Snowpipe makes REST API available to load data from:

	-  Amazon Web Services (AWS) - Amazon S3
	-  Google Cloud Platform - Cloud Storage
	-  Microsoft Azure Blob Storage, and Azure Data Lake Storage Gen2 (ADLS-Gen2)
	-  Hadoop and Spark big data platforms


Environment setup 
-------------------------

First, we need to create the Snowflake account using URL https://signup.snowflake.com/.


.. figure:: images/SnowflakeSubscription.JPG
   :align: center

Snowflake adds a primary sample dataset for this account. 
We can login into the Snowflake with the newly created account.

We can login into the Snowflake with the newly created account. 
After the account is created Snowflake provides the access URL similar to 
"vXXXXXX.east-us-2.azure.snowflakecomputing.com".
This URL can be used to connect to Snowflake web by providing username and password.
Once logged in, we land in the Snowflake account home page as shown below.

.. figure:: images/Snowflakepane.JPG
   :align: center

This interface looks similar to other SQL toolsets, with the navigational menu bar at top of the page. 
We get database pane on the left side, worksheet area in the middle, and results pane at the bottom. 
Worksheet area is used to write and execute the SQL Code. 
Results of the query execution are rendered in the result pane.

Now we are ready to execute our first Snowflake command. Let us add a new database called **IPEDS** here.

Go to worksheet, use the the following command and execute from query pane.

.. code-block:: rst

			CREATE DATABASE IPEDS;

New database is added. 
The IPEDS database appears in the existing collection of Snowflake databases.
Throughout this book IPEDS database is used for data ingestion, transformation, and consumption. 
Before running any SQL commands from this book, always use the newly created database. 

The following command can be executed to change the context of the current 
database and schema to the newly created IPEDS database.

.. code-block:: rst

			USE DATABASE IPEDS;
			USE SCHEMA PUBLIC;

.. note:: 

	*Make sure to change the database context to IPEDS with public schema before executing any SQL from the book*.



SnowSQL CLI Client
^^^^^^^^^^^^^^^^^^^^^^^^^^

We are ready to work in the newly added Snowflake database. Client components are required to connect this 
database from a local machine. Now, let us install Snowflake CLI on our machine. 

After login to Snowflake account, choose help download option to download SnowSQL CLI..

.. figure:: images/SnowflakeCLI.png
   :align: center

Windows version of SnowSQL can also be downloaded from https://sfc-repo.snowflakecomputing.com/snowsql/bootstrap/1.2/windows_x86_64/index.html.
You can find installation and configuration instructions under https://docs.snowflake.com/en/user-guide/snowsql-install-config.html.

The SnowSQL CLI can also be downloaded from:

https://docs.snowflake.com/en/user-guide/snowsql-install-config.html


.. figure:: images/SnowflakeCLI_Source.png
   :align: center


After SnowSQL CLI is installed, go to **Command prompt**. 	
Use the following command to connect Snowflake from the Windows command line.

.. code-block:: rst
     
	snowsql -a accountName -u userName -d databaseName -s schemaName
	 
	snowsql -a newlycreatedsnowflakeAccount -u UserID -d IPEDS -s public
	Password:******

SnowSQL CLI will connect to Snowflake instance. Database interaction can be started
from the SQL prompt as shown below.

.. figure:: images/SnowCLILogin.jpg
   :align: center

We are ready to use database and query. Let us run our first SQL command.

.. code-block:: rst

		SELECT CURRENT_DATABASE(), CURRENT_SCHEMA();

We should get the database and schema names as shown below.

.. figure:: images/SnowCLIResults.JPG
   :align: center

To terminate the current session, run the following command.

.. code-block:: rst

 			!exit


We have successfully created Snowflake account 
and learned how to run queries in Snowflake web using worksheet. 
Also connected to the Snowflake account from our desktop using SnowSQL CLI. 
Now, we are ready to work on our project.


Staged area
----------------------------------

A stage is a temporary storage area in the Snowflake warehouse. Snowflake supports external and internal staged files.

External stages are storage locations outside the Snowflake environment in another cloud storage location. 
It could be Amazon S3 storage, Microsoft Azure Storage, or Google Cloud Storage buckets. This provides greater 
flexibility for accessing the data in Snowflake.  

.. line-block::

	The internal stage stores data files within Snowflake. Internal stages can be either permanent or temporary.  

.. line-block::
	Internal storage is classified into three categories. The data  
	files can be loaded to Snowflake's internal storage using put command from SnowSQL CLI.

User stage  
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Each Snowflake user is attached to the default stage to store the file. The user stage is only accessible by the specific user.

.. code-block:: rst

	put file://D:\Snowflake\sample_file.csv @~/staged;

The file will be uploaded to the user stage and a user can verify the same using **list** command.

.. code-block:: rst

	list @~/staged;

The user stages are referenced using @~.

For example, use **list @~** to list the files in a user stage.

Table stage 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Each table has a Snowflake stage allocated for storing files. Multiple users can access a 
single table stage area.
Table stages are storage locations held within a table object. 
This is helpful when files only need to be loaded into the specific table.
Table stages do not support setting file format options. 

.. code-block:: rst

	put file://D:\Snowflake\sample_file.csv @%test;
	
Users can also specify the subfolder within the table stage to upload the files.

.. code-block:: rst

	put file://D:\Snowflake\sample_file.csv @%test/sample_csv/;

The table stages are referenced using **@%<tableName>**.

For example, to list the files in a table stage:

.. code-block:: rst

	list @%test



Internal named stage
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Internal stage is named database objects. This stage is most recommended for loading. It is available to load 
multiple tables. Multiple users have access to the internal named stage. File formats can be specified while creating an internal named stage.

Internal storage copies data within the Snowflake warehouse as shown below.

.. figure:: images/InternalStorage.jpg
   :align: center

Summary
--------------------------

This chapter has covered Snowflake architectural components, data features, and the various tools used in data loading. 
Multiple ways of connecting to Snowflake services alongside learning about the supported data formats 
are outlined in this chapter. We also created Snowflake account, connected to database, and executed our first set of queries. 
It also summarized various methods for loading files into 
Snowflake using the user stage, table stage, and internal named stage database objects.

In the next chapter, we will cover modern-day data platforms, data formats, and input datasets used throughout this book.

=============================    =============================  =============================
    `&nbsp;&nbsp;<img src= "images/tableofcontents.png" width="25px" height="20px"><font color="blue"><b>Table of contents</b> </font>`_                `&nbsp;&nbsp;<img src= "images/previouschapter.png" width="30px" height="20px"><font color="blue"><b>Previous chapter</b> </font>`_          `&nbsp;&nbsp;<img src= "images/nextchapter.png" width="30px" height="20px"><font color="blue"><b>Next chapter</b> </font>`_                  
=============================    =============================  =============================   

.. _&nbsp;&nbsp;<img src= "images/nextchapter.png" width="30px" height="20px"><font color="blue"><b>Next chapter</b> </font>: ./IntroducingDataplatform.html        
.. _&nbsp;&nbsp;<img src= "images/previouschapter.png" width="30px" height="20px"><font color="blue"><b>Previous chapter</b> </font>: ./Introduction.html        
.. _&nbsp;&nbsp;<img src= "images/tableofcontents.png" width="25px" height="20px"><font color="blue"><b>Table of contents</b> </font>: ./index.html        
